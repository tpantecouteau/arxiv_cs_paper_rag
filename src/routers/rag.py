import asyncio

import nest_asyncio
from fastapi import APIRouter, Depends, HTTPException
from llama_index.core import PromptTemplate, VectorStoreIndex
from llama_index.llms.ollama import Ollama

from ..dependencies import get_index, get_llm

router_rag = APIRouter(prefix="/rag", tags=["RAG"])


@router_rag.get("/query")
def rag_query(
    query: str,
    k: int = 5,
    llm: Ollama = Depends(get_llm),
    index: VectorStoreIndex = Depends(get_index),
):
    """
    Pose une question (query) et renvoie la r√©ponse g√©n√©r√©e √† partir des chunks vectoris√©s.
    """
    try:
        # --- 1Ô∏è‚É£ Setup event loop ---
        nest_asyncio.apply()
        try:
            asyncio.get_event_loop()
        except RuntimeError:
            asyncio.set_event_loop(asyncio.new_event_loop())

        print(f"\nüîç Incoming query: {query}")

        # --- 5Ô∏è‚É£ Prompt personnalis√© ---
        prompt_template = PromptTemplate(
            "You are an expert scientific research assistant. Answer ONLY using the provided context.\n\n"
            "CRITICAL: If the question mentions a specific example (like \"I like fish, especially dolphins\"), "
            "you MUST provide TWO explanations:\n"
            "1. FIRST: Explain the literal contradiction in the example itself (e.g., why dolphins aren't fish)\n"
            "2. SECOND: Explain the broader research problem the paper addresses\n\n"
            "Format your answer with clear sections for each explanation.\n"
            "If information is missing from context, state that explicitly. Do NOT hallucinate.\n\n"
            "Context:\n{context_str}\n\n"
            "Question: {query_str}\n\n"
            "Answer:"
        )

        # --- 6Ô∏è‚É£ Cr√©ation du moteur de recherche ---
        print("üß† Creating query engine...")
        query_engine = index.as_query_engine(
            llm=llm,
            similarity_top_k=k,
            text_qa_template=prompt_template,
            response_mode="compact",
        )

        # --- 7Ô∏è‚É£ Ex√©cution de la requ√™te ---
        print("‚öôÔ∏è Running semantic search & generation...")
        response = query_engine.query(query)

        # --- 8Ô∏è‚É£ Logs d√©taill√©s ---
        print("\n‚úÖ RAG query successful!")
        print(f"üî¢ Retrieved {len(response.source_nodes)} source documents:\n")

        for i, node in enumerate(response.source_nodes, 1):
            meta = node.node.metadata
            text = node.node.text[:800].replace("\n", " ")
            print(f"üìÑ [{i}] Source Metadata: {meta}")
            print(f"   Text Preview: {text}...\n")
            print(f"üìè Chunk length: {len(node.node.text)} chars")
            print(f"üìå Score: {node.score}")

        print("üßæ Final Answer:")
        print(str(response))

        # --- 9Ô∏è‚É£ Retour API ---
        return {
            "query": query,
            "answer": str(response),
            "sources": [
                {
                    "metadata": node.node.metadata,
                    "preview": node.node.text[:800],
                    "score": getattr(node, "score", None),
                }
                for node in response.source_nodes
            ],
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"RAG query failed: {e}")
