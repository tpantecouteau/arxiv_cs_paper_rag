# arXiv RAG Assistant üß†

> AI-powered research paper search and question-answering system using Retrieval-Augmented Generation (RAG)

A production-ready RAG application that enables semantic search and intelligent Q&A over scientific papers from arXiv. Built with modern best practices, optimized ingestion pipeline, and a beautiful dark-themed UI.

![Python](https://img.shields.io/badge/Python-3.10-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115-green)
![React](https://img.shields.io/badge/React-18-61DAFB)
![Airflow](https://img.shields.io/badge/Airflow-2.9-red)

## ‚ú® Features

### üéØ Core Functionality
- **Semantic Search**: Ask natural language questions about research papers
- **Intelligent RAG**: Dual-explanation responses (literal + contextual)
- **Paper Management**: Browse, search, and explore arXiv papers
- **Real-time Q&A**: Fast retrieval with cosine similarity scoring

### üöÄ Technical Highlights
- **Layout-Aware PDF Extraction**: Font-size based header detection with PyMuPDF
- **Structure-Aware Chunking**: Markdown-based semantic chunking preserves document structure
- **Optimized Vector Search**: Cosine similarity on OpenSearch with HNSW indexing
- **Dependency Injection**: Clean architecture with cached components
- **Modern UI/UX**: Glassmorphism dark theme with smooth animations

### üé® Frontend Features
- **Dual-Panel Layout**: Papers list + RAG chat side-by-side
- **Real-time Search**: Filter papers by title or author
- **Rich Responses**: Markdown-formatted answers with source attribution
- **Loading States**: Skeleton loaders and animated transitions
- **Responsive Design**: Mobile, tablet, and desktop optimized

## üèóÔ∏è Architecture

```mermaid
graph LR
    A[arXiv API] -->|Fetch Papers| B[Airflow DAG]
    B -->|Extract Text| C[PyMuPDF]
    C -->|Generate MD| D[Markdown Files]
    D -->|Chunk| E[MarkdownNodeParser]
    E -->|Embed| F[Ollama Embeddings]
    F -->|Index| G[OpenSearch]
    
    H[User Query] -->|Frontend| I[FastAPI Backend]
    I -->|Retrieve| G
    I -->|Generate| J[Ollama LLM]
    J -->|Answer| H
```

### Tech Stack

**Backend**
- **API**: FastAPI + Pydantic for type-safe endpoints
- **RAG**: LlamaIndex for orchestration
- **Vector DB**: OpenSearch with k-NN plugin
- **Embeddings**: Ollama mxbai-embed-large (1024 dim)
- **LLM**: Ollama Mistral 7B
- **Database**: PostgreSQL for metadata

**Ingestion**
- **Orchestration**: Apache Airflow 2.9
- **PDF Parsing**: PyMuPDF with layout detection
- **Chunking**: MarkdownNodeParser for semantic splits
- **Scheduling**: Daily DAG runs

**Frontend**
- **Framework**: React 18 + Vite
- **Styling**: Tailwind CSS with custom design system
- **Icons**: Lucide React
- **State**: React Hooks

## üì¶ Installation

### Prerequisites
- Docker & Docker Compose
- 8GB+ RAM recommended
- Python 3.10+ (for development)

### Quick Start

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd arxiv_paper_rag
```

2. **Configure environment**
```bash
cp .env.example .env
# Edit .env with your settings
```

3. **Start all services**
```bash
docker compose up -d
```

4. **Access the application**
- Frontend: http://localhost:5173
- API: http://localhost:8000
- Airflow: http://localhost:8080 (admin/airflow)
- OpenSearch: http://localhost:9200

## üîß Configuration

### Environment Variables

Create a `.env` file with the following:

```env
# Ollama Configuration
OLLAMA_HOST=http://ollama:11434
OLLAMA_MODEL=mistral
OLLAMA_EMBED_MODEL=mxbai-embed-large

# OpenSearch Configuration
OPENSEARCH_ENDPOINT=http://opensearch:9200
OPENSEARCH_INDEX=paper_chunks_llama
OPENSEARCH_DIM=1024

# PostgreSQL Configuration
POSTGRES_USER=arxiv_user
POSTGRES_PASSWORD=arxiv_pass
POSTGRES_DB=arxiv_db

# arXiv Configuration
ARXIV_SEARCH_QUERY=cat:cs.CL+OR+cat:q-fin
ARXIV_MAX_RESULTS=20
ARXIV_RATE_LIMIT_DELAY=3
```

### Customization

**Change arXiv Categories**
Edit `ARXIV_SEARCH_QUERY` in `.env`:
```env
# Computer Science categories
ARXIV_SEARCH_QUERY=cat:cs.CL+OR+cat:cs.AI+OR+cat:cs.LG

# Physics
ARXIV_SEARCH_QUERY=cat:physics.gen-ph
```

**Adjust Vector Search**
Modify `src/routers/rag.py`:
```python
# Number of chunks to retrieve (default: 5)
k: int = 5  # Reduce for speed, increase for more context
```

**Switch LLM Model**
Update `compose.yml`:
```yaml
environment:
  - OLLAMA_MODEL=phi3:mini  # Faster alternative
```

## üöÄ Usage

### Triggering the Ingestion Pipeline

1. Access Airflow UI: http://localhost:8080
2. Enable the `arxiv_paper_ingestion` DAG
3. Click "Trigger DAG" to start ingestion

The pipeline will:
1. **Fetch** papers from arXiv API
2. **Extract** text with layout awareness (PyMuPDF)
3. **Chunk** text using Markdown structure
4. **Embed** chunks using Ollama
5. **Index** vectors in OpenSearch

### Querying via API

```bash
# Ask a question
curl "http://localhost:8000/rag/query?query=What%20is%20attention%20mechanism?&k=5"

# Response format
{
  "query": "What is attention mechanism?",
  "answer": "Attention mechanism is...",
  "sources": [
    {
      "metadata": {...},
      "preview": "...",
      "score": 0.87
    }
  ]
}
```

### Using the Frontend

1. **Browse Papers**: Left panel shows all papers with search
2. **Ask Questions**: Right panel for natural language queries
3. **View Sources**: Click on sources to see paper details
4. **Keyboard Shortcuts**:
   - `Enter`: Submit query
   - `Shift+Enter`: New line in query
   - `Esc`: Clear input

## üìä Performance Optimizations

### What We've Optimized

‚úÖ **PDF Extraction**
- Font-size based header detection
- Layout-aware column handling
- Markdown output for structure preservation

‚úÖ **Vector Search**
- Cosine similarity (vs L2 distance): ~200x better scores
- HNSW indexing: O(log n) search complexity
- Optimal k=5 chunks: Balanced speed/quality

‚úÖ **RAG Pipeline**
- Dependency injection with `@lru_cache`: Shared instances
- Deterministic document IDs: Prevents duplicates on re-ingestion
- Compact response mode: Faster than tree_summarize

‚úÖ **Frontend**
- Vite for fast HMR and builds
- Skeleton loaders for perceived speed
- CSS transitions hardware-accelerated

### Benchmarks

| Metric | Before | After |
|--------|--------|-------|
| RAG Latency | ~120s | ~60s |
| Score Range | 0.003 | 0.70-0.75 |
| Chunk Quality | ‚ùå Entire document | ‚úÖ Semantic sections |
| UI Load Time | 2s | <500ms |

## üß™ Testing

```bash
# Run backend tests
python -m pytest tests/

# Test ingestion pipeline
python -m pytest tests/test_ingestion.py -v
```

## üêõ Troubleshooting

### Airflow won't start
```bash
# Remove stale PID files
docker compose exec airflow rm -f /opt/airflow/airflow-webserver.pid

# Restart container
docker compose restart airflow
```

### Low retrieval scores
- Ensure you're using **cosine similarity** (not L2)
- Check if index was created correctly:
  ```bash
  curl http://localhost:9200/paper_chunks_llama/_mapping
  ```

### Frontend not connecting to API
- Verify CORS is enabled in `src/main.py`
- Check API is running: `curl http://localhost:8000/`

### Ollama model not found
```bash
# Pull required models
docker compose exec ollama ollama pull mistral
docker compose exec ollama ollama pull mxbai-embed-large
```

## üìÅ Project Structure

```
arxiv_paper_rag/
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ arxiv_ingestion/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ fetch_papers.py       # arXiv API fetcher
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ extract_texts.py      # PyMuPDF extraction
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ chunk_texts.py        # Markdown chunking
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ embed_index.py        # OpenSearch indexing
‚îÇ   ‚îú‚îÄ‚îÄ requirements-airflow.txt
‚îÇ   ‚îî‚îÄ‚îÄ scripts/
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RagChat.jsx          # Q&A interface
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PaperLists.jsx       # Papers browser
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx                  # Main layout
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.css                # Design system
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ papers.py                # Paper CRUD
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag.py                   # RAG endpoint
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ search.py                # Search endpoint
‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py              # DI containers
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Settings
‚îÇ   ‚îî‚îÄ‚îÄ main.py                      # FastAPI app
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_ingestion.py            # Pipeline tests
‚îÇ   ‚îî‚îÄ‚îÄ test_config.py               # Config tests
‚îú‚îÄ‚îÄ compose.yml                      # Docker services
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

## ü§ù Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## üìù License

MIT License - feel free to use this project for your own purposes.

## üôè Acknowledgments

- **LlamaIndex**: RAG orchestration framework
- **Ollama**: Local LLM and embeddings
- **OpenSearch**: Vector database
- **Airflow**: Workflow orchestration
- **arXiv**: Open-access research papers

## üìß Support

For issues or questions:
- Open an issue on GitHub
- Check existing documentation
- Review troubleshooting section
